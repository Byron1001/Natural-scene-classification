{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation utilizes PyTorch to construct the Deep Belief Network(DBN) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.4.0-cp310-cp310-win_amd64.whl (197.9 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.5.1-cp310-cp310-win_amd64.whl (11.0 MB)\n",
      "Collecting jinja2\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.12.2)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.13.2-py3-none-any.whl (6.2 MB)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Collecting numpy>=1.19.5\n",
      "  Using cached numpy-2.1.0-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting scipy>=1.6.0\n",
      "  Using cached scipy-1.14.1-cp310-cp310-win_amd64.whl (44.8 MB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.5-cp310-cp310-win_amd64.whl (17 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: numpy, mpmath, MarkupSafe, threadpoolctl, sympy, scipy, networkx, joblib, jinja2, fsspec, filelock, torch, scikit-learn\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.15.4 fsspec-2024.6.1 jinja2-3.1.4 joblib-1.4.2 mpmath-1.3.0 networkx-3.3 numpy-2.1.0 scikit-learn-1.5.1 scipy-1.14.1 sympy-1.13.2 threadpoolctl-3.5.0 torch-2.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/scikit-learn/\n",
      "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script isympy.exe is installed in 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe and torchrun.exe are installed in 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# in case torch is not installed\n",
    "%pip install torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, pytz, pandas\n",
      "Successfully installed pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# in case pandas was not installed\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp310-cp310-win_amd64.whl (7.8 MB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (24.1)\n",
      "Collecting pillow>=8\n",
      "  Downloading pillow-10.4.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.53.1-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.1.0)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.0-cp310-cp310-win_amd64.whl (216 kB)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-win_amd64.whl (56 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.53.1 kiwisolver-1.4.5 matplotlib-3.9.2 pillow-10.4.0 pyparsing-3.1.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts fonttools.exe, pyftmerge.exe, pyftsubset.exe and ttx.exe are installed in 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# in case matplotlib was not installed\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, jaccard_score\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the paths to train and test datasets\n",
    "TRAIN_DIR = './global-wheat-detection/train/'\n",
    "TEST_DIR = './global-wheat-detection/test/'\n",
    "TRAIN_CSV_PATH = './global-wheat-detection/train.csv'\n",
    "AUG_SAVE_DIR = './global-wheat-detection/augmented_images/'\n",
    "SAVE_PATH = 'models/DBN/'\n",
    "CHECKPOINT_DIR = 'models/DBN/checkpoints/'\n",
    "\n",
    "# Model configuration\n",
    "EPOCHS = 10\n",
    "IMG_SIZE = 256\n",
    "VISIBLE_UNITS = IMG_SIZE * IMG_SIZE\n",
    "HIDDEN_UNITS_1 = 256  # Number of hidden units in the first RBM\n",
    "HIDDEN_UNITS_2 = 128  # Number of hidden units in the second RBM\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 64  # Batch size for training\n",
    "LEARNING_RATE = 0.01  # Learning rate for both pre-training and fine-tuning\n",
    "\n",
    "# Create directories if they don't exist\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 242466 entries, 0 to 242465\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   image_id  242466 non-null  object \n",
      " 1   x         242466 non-null  float64\n",
      " 2   y         242466 non-null  float64\n",
      " 3   w         242466 non-null  float64\n",
      " 4   h         242466 non-null  float64\n",
      " 5   source    242466 non-null  object \n",
      "dtypes: float64(4), object(2)\n",
      "memory usage: 11.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./df_augment.csv')\n",
    "# df = pd.read_csv('./df_full.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing dataframe and image path:   0%|          | 0/242466 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing dataframe and image path: 100%|██████████| 242466/242466 [02:18<00:00, 1745.65it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 262144 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m test_paths, val_paths, test_labels, val_labels \u001b[38;5;241m=\u001b[39m train_test_split(test_paths, test_labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Create PyTorch Datasets\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m create_dataset(val_paths, val_labels)\n\u001b[0;32m     57\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m create_dataset(test_paths, test_labels)\n",
      "Cell \u001b[1;32mIn[26], line 39\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[1;34m(image_paths, labels)\u001b[0m\n\u001b[0;32m     37\u001b[0m targets \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_path, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(image_paths, labels):\n\u001b[1;32m---> 39\u001b[0m     image, target \u001b[38;5;241m=\u001b[39m \u001b[43mload_image_and_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[0;32m     41\u001b[0m     targets\u001b[38;5;241m.\u001b[39mappend(target)\n",
      "Cell \u001b[1;32mIn[26], line 9\u001b[0m, in \u001b[0;36mload_image_and_label\u001b[1;34m(image_id, label, IMG_SIZE)\u001b[0m\n\u001b[0;32m      7\u001b[0m image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# Normalize the image to [0, 1] range\u001b[39;00m\n\u001b[0;32m      8\u001b[0m label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(label, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mtensor(label, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 262144 bytes."
     ]
    }
   ],
   "source": [
    "# Convert image paths and labels to tensors\n",
    "def load_image_and_label(image_id, label, IMG_SIZE=IMG_SIZE):\n",
    "    if not os.path.exists(image_id):\n",
    "        raise FileNotFoundError(f\"File not found: {image_id}\")\n",
    "    image = Image.open(image_id).convert('L')  # Convert to grayscale if necessary\n",
    "    image = image.resize((IMG_SIZE, IMG_SIZE))  # Resize to the desired size\n",
    "    image = np.array(image, dtype=np.float32) / 255.0  # Normalize the image to [0, 1] range\n",
    "    label = np.array(label, dtype=np.int64)\n",
    "    return torch.tensor(image, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "def parse_dataframe(df, TRAIN_DIR=TRAIN_DIR, AUG_SAVE_DIR=AUG_SAVE_DIR):\n",
    "    image_paths = []\n",
    "    bboxes = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc='Parsing dataframe and image path'):\n",
    "        image_id = row['image_id']\n",
    "        bbox = row[['x', 'y', 'w', 'h']].values  # Assuming these are bounding box coordinates\n",
    "        ori_image_path = os.path.join(TRAIN_DIR, f'{image_id}.jpg')\n",
    "        augmented_image_path = os.path.join(AUG_SAVE_DIR, f'{image_id}.jpg')\n",
    "        \n",
    "        # Check if the original or augmented image exists\n",
    "        if os.path.exists(ori_image_path):\n",
    "            image_path = ori_image_path\n",
    "        elif os.path.exists(augmented_image_path):\n",
    "            image_path = augmented_image_path\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Image not found for ID: {image_id}\")\n",
    "        \n",
    "        image_paths.append(image_path)\n",
    "        bboxes.append(bbox)\n",
    "    \n",
    "    return image_paths, bboxes\n",
    "\n",
    "# Create PyTorch datasets\n",
    "def create_dataset(image_paths, labels):\n",
    "    images = []\n",
    "    targets = []\n",
    "    for img_path, label in zip(image_paths, labels):\n",
    "        image, target = load_image_and_label(img_path, label)\n",
    "        images.append(image)\n",
    "        targets.append(target)\n",
    "    images_tensor = torch.stack(images)\n",
    "    labels_tensor = torch.tensor(targets)\n",
    "    dataset = TensorDataset(images_tensor, labels_tensor)\n",
    "    return dataset\n",
    "\n",
    "# Load and preprocess the data\n",
    "image_paths, labels = parse_dataframe(df)\n",
    "\n",
    "# Split the dataset\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(image_paths, labels, test_size=0.4, random_state=42)\n",
    "test_paths, val_paths, test_labels, val_labels = train_test_split(test_paths, test_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create PyTorch Datasets\n",
    "train_dataset = create_dataset(train_paths, train_labels)\n",
    "val_dataset = create_dataset(val_paths, val_labels)\n",
    "test_dataset = create_dataset(test_paths, test_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Inspect the DataLoader\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape, labels.shape, len(train_loader))\n",
    "    break\n",
    "\n",
    "for images, labels in val_loader:\n",
    "    print(images.shape, labels.shape, len(val_loader))\n",
    "    break\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    print(images.shape, labels.shape, len(test_loader))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(history):\n",
    "    epochs = range(1, len(history['loss']) + 1)\n",
    "\n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['loss'], 'r-', label='Training Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['accuracy'], 'b-', label='Training Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\n",
    "\n",
    "# Precision Metric\n",
    "def precision_metric(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Recall Metric\n",
    "def recall_metric(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# F1 Score Metric\n",
    "def f1_metric(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Intersection over Union (IoU) Metric\n",
    "def iou_metric(y_true, y_pred):\n",
    "    return jaccard_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBN_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBNModel:\n",
    "    def __init__(self, visible_units, hidden_units_1, hidden_units_2, n_classes):\n",
    "        self.visible_units = visible_units\n",
    "        self.hidden_units_1 = hidden_units_1\n",
    "        self.hidden_units_2 = hidden_units_2\n",
    "        self.n_classes = n_classes\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Initialize RBMs\n",
    "        rbm1 = RBM(visible_units=self.visible_units, hidden_units=self.hidden_units_1)\n",
    "        rbm2 = RBM(visible_units=self.hidden_units_1, hidden_units=self.hidden_units_2)\n",
    "\n",
    "        # Stack RBMs to form a DBN\n",
    "        dbn = DBN(rbm_layers=[rbm1, rbm2], n_classes=self.n_classes)\n",
    "        return dbn\n",
    "\n",
    "    def compile_model(self, learning_rate=0.01):\n",
    "        # Compile the model by setting up the optimizer and loss function\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def train(self, train_loader, epochs=10, checkpoint_dir=None):\n",
    "        history = {'loss': [], 'accuracy': []}\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            accuracy = correct / len(train_loader.dataset)\n",
    "            history['loss'].append(avg_loss)\n",
    "            history['accuracy'].append(accuracy)\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "            # Save the model if it has the best loss\n",
    "            if checkpoint_dir and avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                self.save(os.path.join(checkpoint_dir, 'dbn_model.pth'))\n",
    "\n",
    "        return history\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                total_loss += loss.item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "                y_true.extend(target.cpu().numpy())\n",
    "                y_pred.extend(pred.cpu().numpy())\n",
    "\n",
    "        avg_loss = total_loss / len(test_loader)\n",
    "        accuracy = correct / len(test_loader.dataset)\n",
    "        precision = precision_metric(y_true, y_pred)\n",
    "        recall = recall_metric(y_true, y_pred)\n",
    "        f1 = f1_metric(y_true, y_pred)\n",
    "        iou = iou_metric(y_true, y_pred)\n",
    "\n",
    "        print(f'Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}, IoU: {iou:.4f}')\n",
    "\n",
    "        return {\n",
    "            'loss': avg_loss,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'iou': iou\n",
    "        }\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        self.model.eval()\n",
    "\n",
    "# Initialize the DBNModel class\n",
    "dbn_model = DBNModel(visible_units=VISIBLE_UNITS, hidden_units_1=HIDDEN_UNITS_1, hidden_units_2=HIDDEN_UNITS_2, n_classes=N_CLASSES)\n",
    "\n",
    "# Compile the model\n",
    "dbn_model.compile_model(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Print model summary (not as straightforward in PyTorch, but showing the structure)\n",
    "print(dbn_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the DBN model\n",
    "history = dbn_model.train(train_loader, epochs=EPOCHS, checkpoint_dir=CHECKPOINT_DIR)\n",
    "\n",
    "# Evaluate the DBN model on the test set\n",
    "test_metrics = dbn_model.evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training metrics\n",
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire DBN model\n",
    "dbn_model.save(SAVE_PATH + 'dbn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DBN model from a file\n",
    "# dbn_model.load(SAVE_PATH + 'dbn_model.pth')\n",
    "\n",
    "# Perform garbage collection\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBN_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden units in the RBM layers are increased to 512 and 256, respectively, which makes this model potentially more capable of capturing complex features but at the cost of increased computational requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DBN model with more hidden units\n",
    "dbn_model_2 = DBNModel(visible_units=VISIBLE_UNITS, \n",
    "                       hidden_units_1=512,  # Increased from 256 to 512\n",
    "                       hidden_units_2=256,  # Increased from 128 to 256\n",
    "                       n_classes=N_CLASSES)\n",
    "\n",
    "# Compile the model\n",
    "dbn_model_2.compile_model(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Print model summary\n",
    "print(dbn_model_2.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history_2 = dbn_model_2.train(train_loader, epochs=EPOCHS, checkpoint_dir=CHECKPOINT_DIR)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_metrics_2 = dbn_model_2.evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training metrics\n",
    "plot_metrics(history_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "dbn_model_2.save(SAVE_PATH + 'dbn_model_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model (if needed)\n",
    "# dbn_model_2.load(SAVE_PATH + 'dbn_model_2.pth')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBN_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A third RBM layer with 64 hidden units is added, making the network deeper. This modification allows the DBN to potentially learn even more abstract features but also requires careful tuning and more data to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DBN model with an additional RBM layer\n",
    "class DBNModelExtended(DBNModel):\n",
    "    def build_model(self):\n",
    "        # Initialize RBMs with an additional layer\n",
    "        rbm1 = RBM(visible_units=self.visible_units, hidden_units=self.hidden_units_1)\n",
    "        rbm2 = RBM(visible_units=self.hidden_units_1, hidden_units=self.hidden_units_2)\n",
    "        rbm3 = RBM(visible_units=self.hidden_units_2, hidden_units=64)  # New additional RBM layer\n",
    "        \n",
    "        # Stack RBMs to form a DBN\n",
    "        dbn = DBN(rbm_layers=[rbm1, rbm2, rbm3], n_classes=self.n_classes)\n",
    "        return dbn\n",
    "\n",
    "# Initialize the DBN model\n",
    "dbn_model_3 = DBNModelExtended(visible_units=VISIBLE_UNITS, \n",
    "                               hidden_units_1=HIDDEN_UNITS_1, \n",
    "                               hidden_units_2=HIDDEN_UNITS_2, \n",
    "                               n_classes=N_CLASSES)\n",
    "\n",
    "# Compile the model\n",
    "dbn_model_3.compile_model(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Print model summary\n",
    "print(dbn_model_3.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history_3 = dbn_model_3.train(train_loader, epochs=EPOCHS, checkpoint_dir=CHECKPOINT_DIR)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_metrics_3 = dbn_model_3.evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training metrics\n",
    "plot_metrics(history_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "dbn_model_3.save(SAVE_PATH + 'dbn_model_3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model (if needed)\n",
    "# dbn_model_3.load(SAVE_PATH + 'dbn_model_3.pth')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[I just did for first DBN model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the training history for the first DBN model\n",
    "with open('dbn_training_history_1.pkl', 'wb') as file:\n",
    "    pickle.dump(history_1, file)\n",
    "\n",
    "# Save the training history for the second DBN model\n",
    "with open('dbn_training_history_2.pkl', 'wb') as file:\n",
    "    pickle.dump(history_2, file)\n",
    "\n",
    "# Save the training history for the third DBN model\n",
    "with open('dbn_training_history_3.pkl', 'wb') as file:\n",
    "    pickle.dump(history_3, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training history for the first DBN model\n",
    "with open('dbn_training_history_1.pkl', 'rb') as file:\n",
    "    loaded_history_1 = pickle.load(file)\n",
    "\n",
    "# Initialize the model again if needed\n",
    "dbn_model = DBNModel(visible_units=VISIBLE_UNITS, \n",
    "                     hidden_units_1=HIDDEN_UNITS_1, \n",
    "                     hidden_units_2=HIDDEN_UNITS_2, \n",
    "                     n_classes=N_CLASSES)\n",
    "\n",
    "# Compile the model\n",
    "dbn_model.compile_model(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Train the model for zero epochs to create an empty history object\n",
    "empty_history_1 = dbn_model.train(train_loader, epochs=0, checkpoint_dir=None)\n",
    "\n",
    "# Set the loaded history to the empty history object\n",
    "empty_history_1 = loaded_history_1\n",
    "\n",
    "# Now empty_history_1 contains the loaded history\n",
    "plot_metrics(empty_history_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning (For DBN_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from optuna import Trial\n",
    "\n",
    "class DBNHyperModel:\n",
    "    def __init__(self, visible_units, n_classes):\n",
    "        self.visible_units = visible_units\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def build(self, trial: Trial):\n",
    "        # Sample hyperparameters\n",
    "        hidden_units_1 = trial.suggest_int('hidden_units_1', 128, 512, step=64)\n",
    "        hidden_units_2 = trial.suggest_int('hidden_units_2', 64, 256, step=64)\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "\n",
    "        # Build the DBN\n",
    "        rbm1 = RBM(visible_units=self.visible_units, hidden_units=hidden_units_1)\n",
    "        rbm2 = RBM(visible_units=hidden_units_1, hidden_units=hidden_units_2)\n",
    "\n",
    "        dbn = DBN(rbm_layers=[rbm1, rbm2], n_classes=self.n_classes)\n",
    "        optimizer = optim.Adam(dbn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        return dbn, optimizer\n",
    "\n",
    "    def objective(self, trial: Trial):\n",
    "        # Build the model\n",
    "        model, optimizer = self.build(trial)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Train the model\n",
    "        model.train()\n",
    "        for epoch in range(EPOCHS):\n",
    "            total_loss = 0\n",
    "            for data, target in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "                    val_loss += loss.item()\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            # Report intermediate objective value\n",
    "            trial.report(val_loss, epoch)\n",
    "\n",
    "            # Handle pruning (optional)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "# Initialize the hypermodel\n",
    "hypermodel = DBNHyperModel(visible_units=VISIBLE_UNITS, n_classes=N_CLASSES)\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='minimize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best hyperparameters\n",
    "best_hyperparameters = study.best_trial\n",
    "\n",
    "# Extract the best learning rate if it was tuned\n",
    "best_learning_rate = best_hyperparameters.params.get('learning_rate', 0.001)  # Default to 0.001 if not tuned\n",
    "\n",
    "# Print the best learning rate to verify\n",
    "print(f\"Best learning rate: {best_learning_rate}\")\n",
    "print(best_hyperparameters.params)\n",
    "\n",
    "# Build the model using the best hyperparameters\n",
    "best_model, best_optimizer = hypermodel.build(best_hyperparameters)\n",
    "\n",
    "# Ensure the optimizer uses the best learning rate\n",
    "for param_group in best_optimizer.param_groups:\n",
    "    param_group['lr'] = best_learning_rate\n",
    "\n",
    "# Example of a learning rate scheduler and early stopping in PyTorch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(best_optimizer, mode='min', factor=0.2, patience=3, min_lr=1e-5)\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "history_best = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    best_model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for data, target in train_loader:\n",
    "        best_optimizer.zero_grad()\n",
    "        output = best_model(data)\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        loss.backward()\n",
    "        best_optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    # Validation loop\n",
    "    best_model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            output = best_model(data)\n",
    "            loss = nn.CrossEntropyLoss()(output, target)\n",
    "            val_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            val_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    # Compute average losses\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    train_accuracy = correct / len(train_loader.dataset)\n",
    "    val_accuracy = val_correct / len(val_loader.dataset)\n",
    "    \n",
    "    # Append to history\n",
    "    history_best['loss'].append(avg_train_loss)\n",
    "    history_best['val_loss'].append(avg_val_loss)\n",
    "    history_best['accuracy'].append(train_accuracy)\n",
    "    history_best['val_accuracy'].append(val_accuracy)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    # Early stopping and learning rate scheduling\n",
    "    scheduler.step(avg_val_loss)\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(best_model.state_dict(), os.path.join(CHECKPOINT_DIR, 'best_dbn_model.pth'))  # Save the best model\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Load the best model\n",
    "best_model.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, 'best_dbn_model.pth')))\n",
    "best_model.eval()\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = best_model(data)\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        test_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = correct / len(test_loader.dataset)\n",
    "\n",
    "print(f'Best model testing Loss: {test_loss:.4f}, Testing Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(history_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "torch.save(best_model.state_dict(), SAVE_PATH + 'best_dbn_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
